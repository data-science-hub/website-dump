
<!DOCTYPE html>
<!--[if lt IE 7]><html class="lt-ie9 lt-ie8 lt-ie7" lang="en" dir="ltr"><![endif]-->
<!--[if IE 7]><html class="lt-ie9 lt-ie8" lang="en" dir="ltr"><![endif]-->
<!--[if IE 8]><html class="lt-ie9" lang="en" dir="ltr"><![endif]-->
<!--[if gt IE 8]><!--><html lang="en" dir="ltr" prefix="content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema#"><!--<![endif]-->
<head>
<meta charset="utf-8" />
<meta name="Generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="estimating-reaction-barriers-deep-reinforcement-learning-0.html" />
<link rel="shortlink" href="../node/878.html" />
<link rel="shortcut icon" href="../sites/default/files/favicon-96x96.png" type="image/png" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<title>Estimating Reaction Barriers with Deep Reinforcement Learning | Data Science</title>
<style type="text/css" media="all">
@import url("https://datasciencehub.net/modules/system/system.base.css?t4kojz");
@import url("https://datasciencehub.net/modules/system/system.menus.css?t4kojz");
@import url("https://datasciencehub.net/modules/system/system.messages.css?t4kojz");
@import url("https://datasciencehub.net/modules/system/system.theme.css?t4kojz");
</style>
<style type="text/css" media="all">
@import url("https://datasciencehub.net/misc/ui/jquery.ui.core.css?t4kojz");
@import url("https://datasciencehub.net/misc/ui/jquery.ui.theme.css?t4kojz");
</style>
<style type="text/css" media="all">
@import url("https://datasciencehub.net/modules/comment/comment.css?t4kojz");
@import url("https://datasciencehub.net/modules/field/theme/field.css?t4kojz");
@import url("../sites/all/modules/logintoboggan/logintoboggan.css%3Ft4kojz.css");
@import url("https://datasciencehub.net/modules/node/node.css?t4kojz");
@import url("https://datasciencehub.net/modules/search/search.css?t4kojz");
@import url("https://datasciencehub.net/modules/user/user.css?t4kojz");
@import url("../sites/all/modules/views/css/views.css%3Ft4kojz.css");
</style>
<style type="text/css" media="all">
@import url("../sites/all/modules/ctools/css/ctools.css%3Ft4kojz.css");
@import url("../sites/all/modules/ctools/css/modal.css%3Ft4kojz.css");
@import url("../sites/default/modules/modal_forms/css/modal_forms_popup.css%3Ft4kojz.css");
@import url("../sites/all/modules/responsive_menus/styles/responsive_menus_simple/css/responsive_menus_simple.css%3Ft4kojz.css");
@import url("../sites/all/modules/date/date_api/date.css%3Ft4kojz.css");
</style>
<style type="text/css" media="screen">
@import url("../sites/all/themes/adaptivetheme/at_core/css/at.settings.style.headings.css%3Ft4kojz.css");
@import url("../sites/all/themes/adaptivetheme/at_core/css/at.settings.style.image.css%3Ft4kojz.css");
@import url("../sites/all/themes/adaptivetheme/at_core/css/at.layout.css%3Ft4kojz.css");
</style>
<style type="text/css" media="all">
@import url("../sites/all/themes/corolla/css/html-elements.css%3Ft4kojz.css");
@import url("../sites/all/themes/corolla/css/forms.css%3Ft4kojz.css");
@import url("../sites/all/themes/corolla/css/tables.css%3Ft4kojz.css");
@import url("../sites/all/themes/corolla/css/page.css%3Ft4kojz.css");
@import url("../sites/all/themes/corolla/css/articles.css%3Ft4kojz.css");
@import url("../sites/all/themes/corolla/css/comments.css%3Ft4kojz.css");
@import url("../sites/all/themes/corolla/css/fields.css%3Ft4kojz.css");
@import url("../sites/all/themes/corolla/css/blocks.css%3Ft4kojz.css");
@import url("../sites/all/themes/corolla/css/navigation.css%3Ft4kojz.css");
@import url("../sites/all/themes/corolla/css/corolla.settings.style.css%3Ft4kojz.css");
@import url("../sites/default/files/color/corolla-80fb2671/colors.css%3Ft4kojz.css");
</style>
<style type="text/css" media="print">
@import url("../sites/all/themes/corolla/css/print.css%3Ft4kojz.css");
</style>
<link type="text/css" rel="stylesheet" href="../sites/default/files/adaptivetheme/corolla_files/corolla.responsive.layout.css%3Ft4kojz.css" media="only screen" />
<style type="text/css" media="screen">
@import url("../sites/default/files/adaptivetheme/corolla_files/corolla.fonts.css%3Ft4kojz.css");
</style>
<link type="text/css" rel="stylesheet" href="../sites/all/themes/corolla/css/responsive.custom.css%3Ft4kojz.css" media="only screen" />
<link type="text/css" rel="stylesheet" href="../sites/all/themes/corolla/css/responsive.smartphone.portrait.css%3Ft4kojz.css" media="only screen and (max-width:320px)" />
<link type="text/css" rel="stylesheet" href="../sites/all/themes/corolla/css/responsive.smartphone.landscape.css%3Ft4kojz.css" media="only screen and (min-width:321px) and (max-width:480px)" />
<link type="text/css" rel="stylesheet" href="../sites/all/themes/corolla/css/responsive.tablet.portrait.css%3Ft4kojz.css" media="only screen and (min-width:481px) and (max-width:768px)" />
<link type="text/css" rel="stylesheet" href="../sites/all/themes/corolla/css/responsive.tablet.landscape.css%3Ft4kojz.css" media="only screen and (min-width:769px) and (max-width:1024px)" />
<link type="text/css" rel="stylesheet" href="../sites/all/themes/corolla/css/responsive.desktop.css%3Ft4kojz.css" media="only screen and (min-width:1025px)" />

<!--[if lt IE 9]>
<style type="text/css" media="screen">
@import url("https://datasciencehub.net/sites/default/files/adaptivetheme/corolla_files/corolla.lt-ie9.layout.css?t4kojz");
</style>
<![endif]-->

<!--[if lte IE 9]>
<style type="text/css" media="screen">
@import url("https://datasciencehub.net/sites/all/themes/corolla/css/ie-lte-9.css?t4kojz");
</style>
<![endif]-->
<script type="text/javascript" src="https://datasciencehub.net/misc/jquery.js?v=1.4.4"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/jquery-extend-3.4.0.js?v=1.4.4"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/jquery-html-prefilter-3.5.0-backport.js?v=1.4.4"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/jquery.once.js?v=1.2"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/drupal.js?t4kojz"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/ui/jquery.ui.core.min.js?v=1.8.7"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/jquery.cookie.js?v=1.0"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/jquery.form.js?v=2.52"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/form-single-submit.js?v=7.98"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/form.js?v=7.98"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/ajax.js?v=7.98"></script>
<script type="text/javascript" src="../sites/default/modules/jquery_update/js/jquery_update.js%3Fv=0.0.1"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/progress.js?v=7.98"></script>
<script type="text/javascript" src="../sites/all/modules/ctools/js/modal.js%3Ft4kojz"></script>
<script type="text/javascript" src="../sites/default/modules/modal_forms/js/modal_forms_popup.js%3Ft4kojz"></script>
<script type="text/javascript" src="../sites/all/modules/responsive_menus/styles/responsive_menus_simple/js/responsive_menus_simple.js%3Ft4kojz"></script>
<script type="text/javascript" src="../sites/all/modules/google_analytics/googleanalytics.js%3Ft4kojz"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga");ga("create", "UA-107867597-1", {"cookieDomain":"auto"});ga("set", "anonymizeIp", true);ga("send", "pageview");
//--><!]]>
</script>
<script type="text/javascript" src="../sites/all/modules/field_group/field_group.js%3Ft4kojz"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/tableheader.js?t4kojz"></script>
<script type="text/javascript" src="https://datasciencehub.net/misc/collapse.js?v=7.98"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","setHasJsCookie":0,"ajaxPageState":{"theme":"corolla","theme_token":"dzwZ0X2xmbIdnyUdb7c6ZzI_DzmiL6pqQwEcALPgWN0","jquery_version":"default","js":{"misc\/jquery.js":1,"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"misc\/ui\/jquery.ui.core.min.js":1,"misc\/jquery.cookie.js":1,"misc\/jquery.form.js":1,"misc\/form-single-submit.js":1,"misc\/form.js":1,"misc\/ajax.js":1,"sites\/default\/modules\/jquery_update\/js\/jquery_update.js":1,"misc\/progress.js":1,"sites\/all\/modules\/ctools\/js\/modal.js":1,"sites\/default\/modules\/modal_forms\/js\/modal_forms_popup.js":1,"sites\/all\/modules\/responsive_menus\/styles\/responsive_menus_simple\/js\/responsive_menus_simple.js":1,"sites\/all\/modules\/google_analytics\/googleanalytics.js":1,"0":1,"sites\/all\/modules\/field_group\/field_group.js":1,"misc\/tableheader.js":1,"misc\/collapse.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/system\/system.theme.css":1,"misc\/ui\/jquery.ui.core.css":1,"misc\/ui\/jquery.ui.theme.css":1,"modules\/comment\/comment.css":1,"modules\/field\/theme\/field.css":1,"sites\/all\/modules\/logintoboggan\/logintoboggan.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/views\/css\/views.css":1,"sites\/all\/modules\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/ctools\/css\/modal.css":1,"sites\/default\/modules\/modal_forms\/css\/modal_forms_popup.css":1,"sites\/all\/modules\/responsive_menus\/styles\/responsive_menus_simple\/css\/responsive_menus_simple.css":1,"sites\/all\/modules\/date\/date_api\/date.css":1,"sites\/all\/themes\/adaptivetheme\/at_core\/css\/at.settings.style.headings.css":1,"sites\/all\/themes\/adaptivetheme\/at_core\/css\/at.settings.style.image.css":1,"sites\/all\/themes\/adaptivetheme\/at_core\/css\/at.layout.css":1,"sites\/all\/themes\/corolla\/css\/html-elements.css":1,"sites\/all\/themes\/corolla\/css\/forms.css":1,"sites\/all\/themes\/corolla\/css\/tables.css":1,"sites\/all\/themes\/corolla\/css\/page.css":1,"sites\/all\/themes\/corolla\/css\/articles.css":1,"sites\/all\/themes\/corolla\/css\/comments.css":1,"sites\/all\/themes\/corolla\/css\/fields.css":1,"sites\/all\/themes\/corolla\/css\/blocks.css":1,"sites\/all\/themes\/corolla\/css\/navigation.css":1,"sites\/all\/themes\/corolla\/css\/fonts.css":1,"sites\/all\/themes\/corolla\/css\/corolla.settings.style.css":1,"sites\/all\/themes\/corolla\/color\/colors.css":1,"sites\/all\/themes\/corolla\/css\/print.css":1,"public:\/\/adaptivetheme\/corolla_files\/corolla.responsive.layout.css":1,"public:\/\/adaptivetheme\/corolla_files\/corolla.fonts.css":1,"sites\/all\/themes\/corolla\/css\/responsive.custom.css":1,"sites\/all\/themes\/corolla\/css\/responsive.smartphone.portrait.css":1,"sites\/all\/themes\/corolla\/css\/responsive.smartphone.landscape.css":1,"sites\/all\/themes\/corolla\/css\/responsive.tablet.portrait.css":1,"sites\/all\/themes\/corolla\/css\/responsive.tablet.landscape.css":1,"sites\/all\/themes\/corolla\/css\/responsive.desktop.css":1,"public:\/\/adaptivetheme\/corolla_files\/corolla.lt-ie9.layout.css":1,"sites\/all\/themes\/corolla\/css\/ie-lte-9.css":1}},"CToolsModal":{"loadingText":"Loading...","closeText":"Close Window","closeImage":"\u003Cimg typeof=\u0022foaf:Image\u0022 class=\u0022image-style-none\u0022 src=\u0022https:\/\/datasciencehub.net\/sites\/all\/modules\/ctools\/images\/icon-close-window.png\u0022 alt=\u0022Close window\u0022 title=\u0022Close window\u0022 \/\u003E","throbber":"\u003Cimg typeof=\u0022foaf:Image\u0022 class=\u0022image-style-none\u0022 src=\u0022https:\/\/datasciencehub.net\/sites\/all\/modules\/ctools\/images\/throbber.gif\u0022 alt=\u0022Loading\u0022 title=\u0022Loading...\u0022 \/\u003E"},"modal-popup-small":{"modalSize":{"type":"fixed","width":500,"height":150},"modalOptions":{"opacity":0.85,"background":"#000"},"animation":"fadeIn","modalTheme":"ModalFormsPopup","throbber":"\u003Cimg typeof=\u0022foaf:Image\u0022 class=\u0022image-style-none\u0022 src=\u0022https:\/\/datasciencehub.net\/sites\/default\/modules\/modal_forms\/images\/loading_animation.gif\u0022 alt=\u0022Loading...\u0022 title=\u0022Loading\u0022 \/\u003E","closeText":"Close"},"modal-popup-medium":{"modalSize":{"type":"fixed","width":550,"height":450},"modalOptions":{"opacity":0.85,"background":"#000"},"animation":"fadeIn","modalTheme":"ModalFormsPopup","throbber":"\u003Cimg typeof=\u0022foaf:Image\u0022 class=\u0022image-style-none\u0022 src=\u0022https:\/\/datasciencehub.net\/sites\/default\/modules\/modal_forms\/images\/loading_animation.gif\u0022 alt=\u0022Loading...\u0022 title=\u0022Loading\u0022 \/\u003E","closeText":"Close"},"modal-popup-large":{"modalSize":{"type":"scale","width":0.8,"height":0.8},"modalOptions":{"opacity":0.85,"background":"#000"},"animation":"fadeIn","modalTheme":"ModalFormsPopup","throbber":"\u003Cimg typeof=\u0022foaf:Image\u0022 class=\u0022image-style-none\u0022 src=\u0022https:\/\/datasciencehub.net\/sites\/default\/modules\/modal_forms\/images\/loading_animation.gif\u0022 alt=\u0022Loading...\u0022 title=\u0022Loading\u0022 \/\u003E","closeText":"Close"},"responsive_menus":[{"toggler_text":"\u2630 Menu","selectors":["#block-system-main-menu"],"media_size":"935","media_unit":"px","absolute":"1","remove_attributes":"1","responsive_menus_style":"responsive_menus_simple"}],"googleanalytics":{"trackOutbound":1,"trackMailto":1,"trackDownload":1,"trackDownloadExtensions":"7z|aac|arc|arj|asf|asx|avi|bin|csv|doc(x|m)?|dot(x|m)?|exe|flv|gif|gz|gzip|hqx|jar|jpe?g|js|mp(2|3|4|e?g)|mov(ie)?|msi|msp|pdf|phps|png|ppt(x|m)?|pot(x|m)?|pps(x|m)?|ppam|sld(x|m)?|thmx|qtm?|ra(m|r)?|sea|sit|tar|tgz|torrent|txt|wav|wma|wmv|wpd|xls(x|m|b)?|xlt(x|m)|xlam|xml|z|zip"},"urlIsAjaxTrusted":{"\/search\/node":true,"\/paper\/estimating-reaction-barriers-deep-reinforcement-learning-0":true},"field_group":{"fieldset":"full"},"ctools-sample-style":{"modalSize":{"type":"fixed","width":1098,"height":500,"addWidth":20,"addHeight":15},"modalOptions":{"opacity":0.5,"background-color":"#000"},"animation":"fadeIn","modalTheme":"CToolsSampleModal","throbber":"\u003Cimg typeof=\u0022foaf:Image\u0022 class=\u0022image-style-none\u0022 src=\u0022https:\/\/datasciencehub.net\/sites\/all\/modules\/ctools\/ctools_ajax_sample\/images\/ajax-loader.gif\u0022 alt=\u0022Loading...\u0022 title=\u0022Loading\u0022 \/\u003E"},"adaptivetheme":{"corolla":{"layout_settings":{"bigscreen":"three-col-grail","tablet_landscape":"three-col-grail","tablet_portrait":"one-col-vert","smalltouch_landscape":"one-col-vert","smalltouch_portrait":"one-col-stack"},"media_query_settings":{"bigscreen":"only screen and (min-width:1025px)","tablet_landscape":"only screen and (min-width:769px) and (max-width:1024px)","tablet_portrait":"only screen and (min-width:481px) and (max-width:768px)","smalltouch_landscape":"only screen and (min-width:321px) and (max-width:480px)","smalltouch_portrait":"only screen and (max-width:320px)"}}}});
//--><!]]>
</script>
<!--[if lt IE 9]>
<script src="https://datasciencehub.net/sites/all/themes/adaptivetheme/at_core/scripts/html5.js?t4kojz"></script>
<![endif]-->
</head>
<body class="html not-front not-logged-in no-sidebars page-node page-node- page-node-878 node-type-submit-paper site-name-hidden atr-7.x-3.x atv-7.x-3.1 lang-en site-name-data-science section-paper color-scheme-custom corolla bs-n bb-n mb-dd rc-6 rct-6">
  <div id="skip-link" class="nocontent">
    <a href="estimating-reaction-barriers-deep-reinforcement-learning-0.html#main-content" class="element-invisible element-focusable">Skip to main content</a>
  </div>
    <div id="page-wrapper">
  <div id="page" class="page ssc-n ssw-n ssa-l sss-n btc-n btw-b bta-l bts-n ntc-n ntw-b nta-l nts-n ctc-n ctw-b cta-l cts-n ptc-n ptw-b pta-l pts-n">

          <div id="menu-bar-wrapper">
        <div class="container clearfix">
          <div id="menu-bar" class="nav clearfix"><nav id="block-system-main-menu" class="block block-system block-menu menu-wrapper menu-bar-wrapper clearfix odd first block-count-1 block-region-menu-bar block-main-menu"  role="navigation">  
      <h2 class="element-invisible block-title">Main menu</h2>
  
  <ul class="menu clearfix"><li class="first leaf menu-depth-1 menu-item-757"><a href="../content/about-data-science.html">About</a></li><li class="leaf menu-depth-1 menu-item-758"><a href="../content/editors.html">Editorial Board</a></li><li class="leaf menu-depth-1 menu-item-1559"><a href="../content/submit-manuscript.html">Submit Manuscript</a></li><li class="leaf menu-depth-1 menu-item-1612"><a href="../content/calls.html">Calls</a></li><li class="leaf menu-depth-1 menu-item-759"><a href="../content/guidelines-authors.html">For Authors</a></li><li class="leaf menu-depth-1 menu-item-760"><a href="../content/guidelines-reviewers.html">For Reviewers</a></li><li class="leaf menu-depth-1 menu-item-582"><a href="../underreview.html" title="Under Review">Under Review</a></li><li class="leaf menu-depth-1 menu-item-583"><a href="../reviewed.html" title="">Reviewed</a></li><li class="last leaf menu-depth-1 menu-item-1611"><a href="../ReviewedAccepted.html" title="">Accepted</a></li></ul>
  </nav><div id="block-orcid-login-orcid-login" class="block block-orcid-login no-title menu-wrapper menu-bar-wrapper clearfix even last block-count-2 block-region-menu-bar block-orcid-login" >  
  
  <p style="background-color: #003b3a; float:right;"><a style="vertical-align: middle; color: white; padding: 10px; display: inline-block; text-decoration: none;" href="https://orcid.org/oauth/authorize?client_id=APP-223SGGDDO35LKOT2&amp;response_type=code&amp;scope=%2Fauthenticate&amp;redirect_uri=https%3A%2F%2Fdatasciencehub.net%2F&amp;state=eyJyZWRpcmVjdCI6InBhcGVyXC9lc3RpbWF0aW5nLXJlYWN0aW9uLWJhcnJpZXJzLWRlZXAtcmVpbmZvcmNlbWVudC1sZWFybmluZy0wIn0%3D"><img alt="ORCID logo" src="../sites/default/files/orcid_24x24.png" style="vertical-align: middle;"/> Log In with ORCID</a></p>
  </div></div>        </div>
      </div>
    
    <div id="header-wrapper">
      <div class="container clearfix">
	        <header class="clearfix with-logo" role="banner">

                      <div id="branding" class="branding-elements clearfix">

                              <div id="logo">
                  <a href="../index.html"><img class="site-logo" typeof="foaf:Image" src="../sites/default/files/DS_LOGO_vertical.png" alt="Data Science" /></a>                </div>
              
                              <div class="element-invisible h-group" id="name-and-slogan">

                                      <h1 class="element-invisible" id="site-name"><a href="../index.html" title="Home page">Data Science</a></h1>
                  
                  
                </div>
              
            </div>
          
          
        </header>

      </div>
    </div>

          <div id="top-panels-wrapper">
        <div class="container clearfix">
          <!-- Three column 3x33 Gpanel -->
          <div class="at-panel gpanel panel-display three-3x33 clearfix">
                                                          </div>
        </div>
      </div>
    
    
    
    <div id="content-wrapper">
      <div class="container">

        <div id="columns">
          <div class="columns-inner clearfix">

            <div id="content-column">
              <div class="content-inner">

                
                <section id="main-content" role="main">

                  
                  <div class="content-margin">
                    <div class="content-style">

                      
                      
                      
                                              <header class="clearfix">
                          <h1 id="page-title">
                            Estimating Reaction Barriers with Deep Reinforcement Learning                          </h1>
                        </header>
                      
                      
                      
                      <div id="content">
                        <div class="region region-content"><div id="block-system-main" class="block block-system no-title odd first last block-count-3 block-region-content block-main" >  
  
  <article id="node-878" class="node node-submit-paper article odd node-lang-en node-full ia-n clearfix" about="/paper/estimating-reaction-barriers-deep-reinforcement-learning-0" typeof="sioc:Item foaf:Document" role="article">
  <div class="node-inner">



              <header class="node-header">

        
        
      </header>
        <span property="dc:title" content="Estimating Reaction Barriers with Deep Reinforcement Learning" class="rdf-meta element-hidden"></span><span property="sioc:num_replies" content="1" datatype="xsd:integer" class="rdf-meta element-hidden"></span>
    <div class="node-content">
        
      <h3>Tracking #: 878-1858</h3>          								<fieldset>
					<!-- legend>
						<span class="fieldset-legend">Paper Data</span>
					</legend -->

					


					
					<div style="background-color: #E3E5E3;border-radius: 5px;padding-top:4px;padding-bottom:4px;padding-left:6px;"><h5>Authors:</h5><br/><form action="estimating-reaction-barriers-deep-reinforcement-learning-0.html" method="post" id="authors-table-form-anonymus" accept-charset="UTF-8"><div><div id="authors_Container"><table class="sticky-enabled">
 <thead><tr><th></th><th>Name</th><th>ORCID</th> </tr></thead>
<tbody>
 <tr class="odd"><td><div class="form-item form-type-checkbox form-item-authors-2320">
  <input type="checkbox" id="edit-authors-2320" name="authors[2320]" value="2320" class="form-checkbox" />
</div>
</td><td>Adittya Pal</td><td><img alt="ORCID logo" src="../sites/default/files/orcid_24x24.png" style="vertical-align: middle; margin:0px;"/> <a href="https://orcid.org/0009-0005-0705-7768" target="_blank">https://orcid.org/0009-0005-0705-7768</a></td> </tr>
</tbody>
</table>
</div><input type="hidden" name="author_nd" value="878" />
<input type="hidden" name="author_vd" value="1858" />
<input type="hidden" name="form_build_id" value="form-jqDxzpVP3dAiwLK0bddQZq3iY3LKRbkKo2dKT9DWtJ0" />
<input type="hidden" name="form_id" value="authors_table_form_anonymus" />
</div></form><hr/></div><br/>

					<div style="background-color: #E3E5E3;border-radius: 5px;padding-top:4px;padding-bottom:4px;padding-left:6px;"><section class="field field-name-field-editor field-type-user-reference field-label-above view-mode-full"><h2 class="field-label">Responsible editor:&nbsp;</h2><div class="field-items"><div class="field-item even">Richard Mann</div></div></section></div><br/>
					                    <div style="background-color: #E3E5E3;border-radius:5px;padding-top:4px;padding-bottom:4px;padding-left:6px;">
					<section class="field field-name-field-submission-type field-type-list-text field-label-above view-mode-full"><h2 class="field-label">Submission Type:&nbsp;</h2><div class="field-items"><div class="field-item even">Research Paper</div></div></section><section class="field field-name-field-abstract field-type-text-long field-label-above view-mode-full"><h2 class="field-label">Abstract:&nbsp;</h2><div class="field-items"><div class="field-item even">Stable states in complex systems correspond to local minima on the associated potential energy surface. Transitions between these local minima govern the dynamics of such systems. Precisely determining the transition pathways in complex and high-dimensional systems is challenging because these transitions are rare events, and isolating the relevant species in experiments is difficult. Most of the time, the system remains near a local minimum, with rare, large fluctuations leading to transitions between minima. The probability of such transitions decreases exponentially with the height of the energy barrier, making the system&#039;s dynamics highly sensitive to the calculated energy barriers. This work aims to formulate the problem of finding the minimum energy barrier between two stable states in the system&#039;s state space as a cost-minimization problem. It is proposed to solve this problem using reinforcement learning algorithms. The exploratory nature of reinforcement learning agents enables efficient sampling and determination of the minimum energy barrier for transitions.</div></div></section><section class="field field-name-field-pdf field-type-file field-label-above view-mode-full"><h2 class="field-label">Manuscript:&nbsp;</h2><div class="field-items"><div class="field-item even"><span class="file"><img class="file-icon" alt="PDF icon" title="application/pdf" src="https://datasciencehub.net/modules/file/icons/application-pdf.png" /> <a href="../system/files/ds-paper-878.pdf" type="application/pdf; length=1735679">ds-paper-878.pdf</a></span></div></div></section><section class="field field-name-field-previous-version field-type-node-reference field-label-above view-mode-full"><h2 class="field-label">Previous Version:&nbsp;</h2><div class="field-items"><div class="field-item even"><a href="estimating-reaction-barriers-deep-reinforcement-learning.html">Estimating Reaction Barriers with Deep Reinforcement Learning</a></div></div></section><section class="field field-name-field-tags field-type-taxonomy-term-reference field-label-above view-mode-full"><h2 class="field-label">Tags:&nbsp;</h2><ul class="field-items"><li class="field-item even">Reviewed</li></ul></section><section class="field field-name-field-data-repository-urls field-type-text-long field-label-above view-mode-full"><h2 class="field-label">Data repository URLs:&nbsp;</h2><div class="field-items"><div class="field-item even"><div class="field-items">
<div class="field-item even">
<p>GitHub repository for the code: <a href="https://github.com/AdittyaPal/energy_barrier_rl">https://github.com/AdittyaPal/energy_barrier_rl</a></p>
<p>.ipynb file for figures: <a href="https://github.com/AdittyaPal/energy_barrier_rl/blob/main/figures.ipynb">https://github.com/AdittyaPal/energy_barrier_rl/blob/main/figures.ipynb</a></p>
<p>Zenodo repository for trajectories and plot data: Pal, A. (2024). Supporting Data for the submission Estimating Reaction Barriers using Deep Reinforcement Learning. Zenodo. <a href="https://doi.org/10.5281/zenodo.12783976">https://doi.org/10.5281/zenodo.12783976</a></p>
</div>
</div>
</div></div></section><section class="field field-name-field-date-of-submission field-type-datetime field-label-above view-mode-full"><h2 class="field-label">Date of Submission:&nbsp;</h2><div class="field-items"><div class="field-item even"><span class="date-display-single" property="dc:date" datatype="xsd:dateTime" content="2024-09-04T00:00:00-04:00">Wednesday, September 4, 2024</span></div></div></section><section class="field field-name-field-date-of-decision field-type-datetime field-label-above view-mode-full"><h2 class="field-label">Date of Decision:&nbsp;</h2><div class="field-items"><div class="field-item even"><span class="date-display-single" property="dc:date" datatype="xsd:dateTime" content="2024-09-23T00:00:00-04:00">Monday, September 23, 2024</span></div></div></section><br><br><b>Nanopublication URLs:</b> <br><a href="http://ds.kpxl.org/RAZdadPk2Lz1Bo6jvZxCeFME9-Yr-NXwBxOJuOxgWfVSk">http://ds.kpxl.org/RAZdadPk2Lz1Bo6jvZxCeFME9-Yr-NXwBxOJuOxgWfVSk</a><br>                    </div>
                    <br/>

					
                    
                    <div style="background-color: #E3E5E3;border-radius: 5px;padding-top:4px;padding-bottom:4px;padding-left:6px;"><section class="field field-name-field-decision field-type-list-text field-label-above view-mode-full"><h2 class="field-label">Decision:&nbsp;</h2><div class="field-items"><div class="field-item even">Accept</div></div></section></div><br/>                    <div id="review">
                    		                    	</div>

					
										<div style="background-color: #E3E5E3;border-radius: 5px;padding-top:4px;padding-bottom:4px;padding-left:6px;"><b>Solicited Reviews:</b><br/><form action="estimating-reaction-barriers-deep-reinforcement-learning-0.html" method="post" id="get-submitted-reviews" accept-charset="UTF-8"><div><fieldset class="collapsible collapsed form-wrapper" id="edit-fset1"><legend><span class="fieldset-legend">Review #1 <strong>submitted on 11/Sep/2024</strong></span></legend><div class="fieldset-wrapper with-legend"><div class="fieldset-description">By <strong>Chris Beeler</strong> <img alt="ORCID logo" src="../sites/default/files/orcid_24x24.png" style="vertical-align: middle; margin:0px;"/> <a href="https://orcid.org/0000-0002-8603-3027" target="_blank">https://orcid.org/0000-0002-8603-3027</a><br/><br/>Review Details<br/><hr/></div><p>Reviewer has chosen <b>not</b> to be <strong><em>Anonymous</em></strong></p><b style="color:#f39222">Overall Impression:</b> Good</br><b style="color:#f39222">Suggested Decision: </b> Undecided</br><b style="color:#f39222">Technical Quality of the paper:</b> Good<br/><b style="color:#f39222">Presentation:</b> Good<br/><b style="color:#f39222">Reviewer`s confidence:</b> High<br/><b style="color:#f39222">Significance:</b> Moderate significance<br/><b style="color:#f39222">Background:</b> Comprehensive<br/><b style="color:#f39222">Novelty:</b> Limited novelty<br/><b style="color:#f39222">Data availability:</b> All used and produced data (if any) are FAIR and openly available in established data repositories<br/><b style="color:#f39222">Length of the manuscript:</b> The length of this manuscript is about right</p><p><b style="color:#f39222">Summary of paper in a few sentences (summary of changes and improvements for
second round reviews): </b><br/><p>This paper aims to estimate reaction barrier energies via a TD3-SAC hybrid reinforcement learning method. They first do this by constructing the potential energy landscapes as OpenAI gymnasium style environments where the agent can navigate through this space like a maze, creating some reaction pathway. The goal they set for the agent to learn is minimizing the sum of energies for each state visited in this pathway. They show that their algorithm is able to estimate near optimal reaction barriers in two examples.</p>
</p><p><b style="color:#f39222">Reasons to accept: </b><br/><p>The problem this paper focuses on is interesting and a solution would have a clear application.<br />
A new RL algorithm is presented that attempts to improve upon TD3 and SAC.<br />
The author has clearly outlined the limitations of the work presented.<br />
The author discusses the results in great detail.</p>
</p><p><b style="color:#f39222">Reasons to reject: </b><br/><p>Results are only presented for two 2D examples of the problem here.</p>
</p><p><b style="color:#f39222">Nanopublication comments: </b><br/></p><p><b style="color:#f39222">Further comments: </b><br/><p>While the author has outlined that other state-of-the-art studies mostly only focus on 2D Müller–Brown potential surfaces, one would need to apply this algorithm to more than two examples to extract any meaningful performance metrics on it. in my opinion.</p>
<p>Overall the manuscript has been significantly improved from the initial version. Except the one listed above, all my previous concerns have been addressed in detail. While it is just the one comment I have left, I consider it quite a big concern, hence why I have remained undecided. If the author were to add additional results (just a single figure would do), summarizing the performance of this algorithm trained on several different other Müller–Brown surfaces, this would shift me to suggest acceptance.</p>
<p>Alternatively, if the editor or other reviewer feel that the results presented are sufficient, then I will not press the issue any further.</p>
</p></div></fieldset>
<fieldset class="collapsible collapsed form-wrapper" id="edit-fset2"><legend><span class="fieldset-legend">Review #2 <strong>submitted on 19/Sep/2024</strong></span></legend><div class="fieldset-wrapper with-legend"><div class="fieldset-description">By <strong>Stephen Gow</strong> <img alt="ORCID logo" src="../sites/default/files/orcid_24x24.png" style="vertical-align: middle; margin:0px;"/> <a href="https://orcid.org/0000-0003-0121-1697" target="_blank">https://orcid.org/0000-0003-0121-1697</a><br/><br/>Review Details<br/><hr/></div><p>Reviewer has chosen <b>not</b> to be <strong><em>Anonymous</em></strong></p><b style="color:#f39222">Overall Impression:</b> Good</br><b style="color:#f39222">Suggested Decision: </b> Accept</br><b style="color:#f39222">Technical Quality of the paper:</b> Average<br/><b style="color:#f39222">Presentation:</b> Good<br/><b style="color:#f39222">Reviewer`s confidence:</b> Medium<br/><b style="color:#f39222">Significance:</b> Moderate significance<br/><b style="color:#f39222">Background:</b> Reasonable<br/><b style="color:#f39222">Novelty:</b> Clear novelty<br/><b style="color:#f39222">Data availability:</b> All used and produced data (if any) are FAIR and openly available in established data repositories<br/><b style="color:#f39222">Length of the manuscript:</b> The length of this manuscript is about right</p><p><b style="color:#f39222">Summary of paper in a few sentences (summary of changes and improvements for
second round reviews): </b><br/><p>The reviews paper includes several changes from the original submission. Additional background content on reinforcement learning, discussion of relevant prior work and references have been added. The presentation and ordering of figures have been revised, and the discussion around them expanded and moved from captions to the body of the text. Additional consideration of future directions for the developed method and its strengths and weaknesses is included in the conclusions section, which has been split from the discussion of related work. A number of typos and small errors have been corrected.</p>
</p><p><b style="color:#f39222">Reasons to accept: </b><br/><p>As before, the paper provides a useful and novel contribution to a problem with real significance. I appreciate the efforts of the author to improve the paper since the first review, which have substantially improved its clarity and links to previous work.</p>
</p><p><b style="color:#f39222">Reasons to reject: </b><br/><p>I remain concerned that the implementation choices in the reinforcement learning algorithm, and their effect on the behaviour of the agent, are not sufficiently well described to be of use to researchers who may wish to use this method in future. In particular, the importance of the scaling parameter λ is highlighted twice in the response to reviewers, under both my point 1 and my fellow reviewer's point 9. This is important content which should be discussed in the paper itself - at present it is not obvious from the paper that varying this parameter was even considered.</p>
</p><p><b style="color:#f39222">Nanopublication comments: </b><br/></p><p><b style="color:#f39222">Further comments: </b><br/><p>The expansion of the third paragraph of the introduction is broadly welcome, but it is now too long to read comfortably and covers a wide range of somewhat disparate content - it should be split into two paragraphs for ease of understanding.</p>
</p></div></fieldset>
<input type="hidden" name="form_build_id" value="form-sa00y-Se1UYssqsGo_MREwp8jfwvz8wYw9xRgDPaxdI" />
<input type="hidden" name="form_id" value="get_submitted_reviews" />
</div></form></div><br/>                    
					<div style="background-color: #E3E5E3;border-radius: 5px;padding-top:4px;padding-bottom:4px;padding-left:6px;"><fieldset class="collapsible collapsed group-response-to-reviews field-group-fieldset form-wrapper"><legend><span class="fieldset-legend">RESPONSE TO REVIEWERS</span></legend><div class="fieldset-wrapper with-legend"><div class="field field-name-field-response-to-reviews field-type-text-long field-label-hidden view-mode-full"><div class="field-items"><div class="field-item even"><p>The text added in the re-submission is in red. The count of works in the .tex file was ~9000, which was below the target of 12000 words for a submission. I thank reviewer 1 for the positive comments. The reply to the reviewer's suggestions are listed below. The choice of the values for the parameters \tau and \gamma, and the neural network architecture were kept the same as the TD3 (<a href="https://github.com/sfujim/TD3/tree/master">https://github.com/sfujim/TD3/tree/master</a>) and SAC (<a href="https://github.com/haarnoja/sac/tree/master">https://github.com/haarnoja/sac/tree/master</a>) algorithm implementations. The scaling factor \lambda adjusts the step size for the agent, and it was this combination of the scaling factor and number of steps in an episode which led to the best performance of the agent. The agent reaches near the terminal state with just the number of required small enough steps to end the episode. A larger value of \lambda resulted in the agent taking longer steps over regions of the potential energy surface with a higher energy to give an incorrect estimate of the barrier height. Smaller values of \lambda led to smaller steps, and the agent did not leave the local minima to explore other regions of the potential energy surface, and is unsuccessful a its assigned task. It is difficult to visualizing these effects as a plot of the rewards against the number of validation steps as those in Figure 6. The captions of all the figures, and Figure 1, 3 and 4 in particular have been shortened. The typos in the caption of Figure 2 has been corrected. A sentence introducing the actor and critic functions has been added (Page 5, lines 14-18). The three pathways are labeled and the labels are used to identify the energy profiles of the respective pathways. I thank reviewer 2 for the constructive comments and suggestions. The reply to the reviewer's suggestions are listed below. Introduction: More citations have been added while introduction reinforcement learning in Section 1, which are: - Reinforcement Learning: A Survey (<a href="https://doi.org/10.48550/arXiv.cs/9605103">https://doi.org/10.48550/arXiv.cs/9605103</a>) - P. Maes, "Modeling Adaptive Autonomous Agents," in Artificial Life, vol. 1, no. 1_2, pp. 135-162, Oct. 1993, doi: 10.1162/artl.1993.1.1_2.135. - Reinforcement Learning: An Introduction by Sutton and Barto was cited in the previous submission. - Reward is enough (<a href="https://doi.org/10.1016/j.artint.2021.103535">https://doi.org/10.1016/j.artint.2021.103535</a>) - T. Mannucci and E. -J. van Kampen, "A hierarchical maze navigation algorithm with Reinforcement Learning and mapping," 2016 IEEE Symposium Series on Computational Intelligence (SSCI), 2016, pp. 1-8, doi: 10.1109/SSCI.2016.7849365, was cited in the previous submission. - D. Osmanković and S. Konjicija, "Implementation of Q — Learning algorithm for solving maze problem," 2011 Proceedings of the 34th International Convention MIPRO, 2011, pp. 1619-1622. - M. A. Wiering and H. van Hasselt, "Ensemble Algorithms in Reinforcement Learning," in IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 38, no. 4, pp. 930-936, Aug. 2008, doi: 10.1109/TSMCB.2008.920231. - Neural Map: Structured Memory for Deep Reinforcement Learning (<a href="https://doi.org/10.48550/arXiv.1702.08360">https://doi.org/10.48550/arXiv.1702.08360</a>) - Brunner, G., Richter, O., Wang, Y., &amp; Wattenhofer, R. (2018). Teaching a Machine to Read Maps With Deep Reinforcement Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). <a href="https://doi.org/10.1609/aaai.v32i1.11645">https://doi.org/10.1609/aaai.v32i1.11645</a> Figure 1 has been downsized to occupy less space and its source is acknowledged. While it was a little difficult to make an analogy between the actions and rewards (especially rewards, because the agent has different objectives in both environments), it has been added at lines 5-8 and 11-16 on page 3. The typo in the caption of Figure 2 was removed and most of the caption is incorporated into the main text. Methods: The typo in the equation for Rt (extra comma). (Pg. 6 Ln. 43) was corrected. The missing \gamma in the state-action function (Pg. 8 Ln. 6) was added. A sentence introducing the actor and critic functions has been added on page 5, lines 14-18. Target policy smoothing is introduced on page 8 lines 26-34. The author acknowledges that formulated MDP suffers from the problem raised by the reviewer. The number of steps in an episode, the scaling factor of actions and the number of training epochs were varied to come up with a set of values for these three parameters which minimize the problems cause due to the imperfect formulation of the MDP. To discourage the agent from sitting in some state \delta away from the target state, and collect rewards for the remainder of the episode, the episode is truncated after 500 steps. A small scaling factor \lambda was used for the actions, so that the agent does not make too many long jumps through higher energy states to reach a state with lower energy faster. Decreasing \lambda would require increasing the maximum number of steps in an episode, so that the agent explores regions away from the starting point, but not too much that the trajectory passes through regions with higher energy. The lowest reward in the episode (corresponding to the highest energy along the pathway, plotted in added Figure 5b, was monitored to decide when the agent stops improving at its intended task. Using the model after 1000 validation steps indeed led to a higher estimate of the energy barrier. Experiments: Figure 3b was shifted to be a part of Figure 5, as suggested by the reviewer. I would like to acknowledge my mistake of not multiplying the discount factor \gamma while calculating the returns from the episodes. It has been corrected and leads to a much flatter learning curve. Some text is added on Pages 10 and 11 to elaborate on the plots in Figure 4. The table in Figure 6 was updated after multiplying the discount factor while calculating the average rewards. Text was added to page 16, lines 33 onward, to elaborate on the results from Figure 7. However, it was kept a part of conclusions because it demonstrates a conclusion: using a reinforcement learning based approach has an advantage compared to the existing gradient based algorithms. The section has been split. Discussions contain comparison of the current work with previous work, while conclusions focus only on this work. I would like to thank out the reviewer for pointing out relevant references. A discussion of these in the context of the current work is added in Section 4 and summarized below: - Khan, Ahmad, and Alexei Lapkin. "Searching for optimal process routes: A reinforcement learning approach." Computers &amp; Chemical Engineering 141 (2020): 107027. This work focuses on maximizing the profit (as understood from Figure 5) of a sequence of reactions, in a discrete thermodynamic state space represented by a thermodynamic graph. The current work focuses on determining an estimate of energy barrier for a single reaction in a continuous state space. - Zhang, Chonghuan, and Alexei A. Lapkin. "Reinforcement learning optimization of reaction routes on the basis of large, hybrid organic chemistry–synthetic biological, reaction network data." Reaction Chemistry &amp; Engineering 8.10 (2023): 2491-2504. This work uses reinforcement learning to minimize the cost of a sequence of reactions (called synthesis plans) with respect to the price of the starting molecules and atom economy of individual reactions. Figure 2b suggests that the state space is discrete (albeit large), which allows the use to tabular learning algorithms, which cannot be used for continuous state spaces. - Lan, Tian, and Qi An. "Discovering catalytic reaction networks using deep reinforcement learning from first-principles." Journal of the American Chemical Society 143.40 (2021): 16804-16812. and Lan, T., Wang, H. &amp; An, Q. Enabling high throughput deep reinforcement learning with first principles to investigate catalytic reaction mechanisms. Nat Commun 15, 6281 (2024). <a href="https://doi.org/10.1038/s41467-024-50531-6">https://doi.org/10.1038/s41467-024-50531-6</a> These works use deep reinforcement learning in a 23 dimensional discrete state space to determine the best pathway consisting of a sequence of reactions (as shown in Figure 4). The energy barriers for individual reactions is determined using a software called VASP. The objective of the current work is to determine the energy barrier for a single reaction using deep reinforcement learning. - Zhang, Jun, et al. "Deep reinforcement learning of transition states." Physical Chemistry Chemical Physics 23.11 (2021): 6888-6895. This work was already cited in the previous submission. This work formulates the search for transition states as a shooting game from some configuration in the state space with randomized momenta for the two trajectories in opposite directions, expecting them to converge at the two minima representing the two sides of the reaction. It demonstrates the method on 4 two dimensional environments (in the three higher dimensional environments, two dimensions of interest have been chosen). The current work starts from a local minima and tries to learn a trajectory to another minima using reinforcement learning and read off the energy barrier of the transition from the energies along the generated trajectory. - Zhou, Zhenpeng, Xiaocheng Li, and Richard N. Zare. "Optimizing chemical reactions with deep reinforcement learning." ACS central science 3.12 (2017): 1337-1344. This work tries to optimize chemical reactions by perturbing the experimental conditions to achieve a better measure of selectivity, purity or cost for the reaction, using deep reinforcement learning, which has application is the laboratory. It does not estimate the energy barrier for a reaction. - Alexis W. Mills, et al. "Exploring Potential Energy Surfaces Using Reinforcement Machine Learning" Journal of Chemical Information and Modeling 2022 62 (13), 3169-3179, DOI: 10.1021/acs.jcim.2c00373 This work demonstrates the use of a modified DDPG algorithm to determine the minima on a potential energy surface. The current work assumes that the local minima are already known and attempts to estimate the energy barrier between the transition between two minima. I acknowledge that two-dimensional (simpler) environments have been as examples. Higher dimensional state spaces would require more computational resources and longer training-times for the agent to learn. I would like to point out that most state-of-the art works also use two-dimensional models. One of the references suggested bu the reviewer, Zhang, Jun, et al. Deep reinforcement learning of transition states." Physical Chemistry Chemical Physics 23.11 (2021): 6888-6895, uses four two dimensional models, all with two potential wells. For systems with multiple dimensions, two dimensions have been chosen (by expert knowledge, called order parameters), and the agent used only those two dimensions. To avoid this (human) choice, environments with only two-dimensions were used in the current work. In works where the state space has a higher number of dimensions, the state space is discrete. While I admit that the Mueller–Brown potential is a constructed artificial potential, nevertheless it had been used to show the effectiveness of the algorithms is use to determine minimum energy pathways: - Growing String Methods: Wolfgang Quapp; A growing string method for the reaction pathway defined by a Newton trajectory. J. Chem. Phys. 1 May 2005; 122 (17): 174106. (<a href="https://doi.org/10.1063/1.1885467">https://doi.org/10.1063/1.1885467</a>) - Nudged Elastic Band: Graeme Henkelman, Hannes Jónsson; Improved tangent estimate in the nudged elastic band method for finding minimum energy paths and saddle points. J. Chem. Phys. 8 December 2000; 113 (22): 9978–9985. (<a href="https://doi.org/10.1063/1.1323224">https://doi.org/10.1063/1.1323224</a>) used a two dimensional LEPS model potential with two minima only (Mueller–Brown potential has an intermediate third minima). - Baron Peters, Andreas Heyden et. al, A growing string method for determining transition states: Comparison to the nudged elastic band and string methods. J. Chem. Phys. 1 May 2004; 120 (17): 7877–7886. (<a href="https://doi.org/10.1063/1.1691018">https://doi.org/10.1063/1.1691018</a>) - Accelerated Molecular Dynamics: Adaptively Accelerating Reactive Molecular Dynamics Using Boxed Molecular Dynamics in Energy Space, Robin J. Shannon, Silvia Amabilino et. al, Journal of Chemical Theory and Computation 2018 14 (9), 4541-4552. (<a href="https://doi.org/10.1021/acs.jctc.8b00515">https://doi.org/10.1021/acs.jctc.8b00515</a>) - Artificial Force Induced Reaction: Quapp W, Bofill JM, Mechanochemistry on the Mueller–Brown surface by Newton trajectories. Int J Quantum Chem. 2018;118:e25522. (<a href="https://doi.org/10.1002/qua.25522">https://doi.org/10.1002/qua.25522</a>) - Reinforcement Learning: Exploring Potential Energy Surfaces Using Reinforcement Machine Learning, Alexis W. Mills, Joshua J. Goings et. al, Journal of Chemical Information and Modeling 2022 62 (13), 3169-3179, (<a href="https://doi.org/10.1021/acs.jcim.2c00373">https://doi.org/10.1021/acs.jcim.2c00373</a>) uses a RL agent to explore the potential energy surface. The reply to the reviewers has also been attached as the last 6 pages of the submission.</p>
</div></div></div></div></fieldset>
</div><br/>
				</fieldset>
				
    </div>

    <div id="admin-review">
			</div>

	<div id="decision">
			</div>




          <nav class="clearfix"><ul class="links inline"><li class="comment_forbidden first"><span><a href="../user/login%3Fdestination=comment%252Freply%252F878%23comment-form.html">Log in</a> to post comments</span></li><li class="camera_ready_files_field_paper last"></li></ul></nav>
    
    <section id="comments" class="comment-wrapper">

          <h2 class="title">
      1 Comment    </h2>
      
  <a id="comment-165"></a>
<article class="comment odd first last clearfix" about="/comment/165#comment-165" typeof="sioc:Post sioct:Comment">

  
  
      <header class="comment-header">
 
          <h3 property="dc:title" datatype="" class="comment-title"><a href="../comment/165.html#comment-165" rel="bookmark">meta-review by editor</a></h3>
    
        
    <p class="comment-submitted"><span property="dc:date dc:created" content="2024-09-23T02:49:44-04:00" datatype="xsd:dateTime" rel="sioc:has_creator">Submitted by <span class="username" xml:lang="" about="/users/tobias-kuhn" typeof="sioc:UserAccount" property="foaf:name" datatype="">Tobias Kuhn</span> on <time datetime="2024-09-23T02:49:44Z"><span class="date-time">Mon, 09/23/2024 - 02:49</span></time></span><p>
  </header>
    
  <div class="comment-content">
    <span rel="sioc:reply_of" resource="/paper/estimating-reaction-barriers-deep-reinforcement-learning-0" class="rdf-meta element-hidden"></span><div class="field field-name-comment-body field-type-text-long field-label-hidden view-mode-full"><div class="field-items"><div class="field-item even" property="content:encoded"><p>Both reviewers agree that the revised manuscript is substatially improved from the initial submission. From these reviews and my reading of the author response to the initial reviews I am happy to accept the manuscript for publication, subject to one remaining minor revision. Please include in the manuscript itself an overview of the effect of varying the scaling parameter lambda; as mentioned here by reviewer #2 this belongs in the manuscript as well as the response to reviewers. I invite you also to consider addressing the remaining comment by reviewer #1 regarding presenting the result of applying the algorithm to other Müller-Brown surfaces. Please add results as suggested if this can be done straightforwardly and if you agree that it would improve the paper, but my recommendation of acceptance does not depend on this.</p>
<p>Richard Mann (<a href="https://orcid.org/0000-0003-0701-1274">https://orcid.org/0000-0003-0701-1274</a>)</p>
</div></div></div>  </div>

  
      <nav class="clearfix"><ul class="links inline"><li class="comment_forbidden first last"><span><a href="../user/login%3Fdestination=comment%252Freply%252F878%23comment-form.html">Log in</a> to post comments</span></li></ul></nav>
  
</article>

  
</section>

  </div>
  </article>

  </div></div>                      </div>

                      
                    </div>
                  </div>

                </section>

                
              </div>
            </div>

                        
          </div>
        </div>

      </div>
    </div>

    
    
          <div id="footer-wrapper">
        <div class="container clearfix">
          <footer class="clearfix" role="contentinfo">
            <div class="region region-footer"><div class="region-inner clearfix"><div id="block-block-16" class="block block-block no-title odd first last block-count-4 block-region-footer block-16" ><div class="block-inner clearfix">  
  
  <div class="block-content content"><p style="text-align:center;"><a href="https://www.iospress.nl/disclaimer/" target="_blank">Disclaimer</a> | <a href="https://content.iospress.com/page/privacy" target="_blank">Privacy Policy</a> | Data Science is Published by <a href="http://www.iospress.com" target="_blank">IOS Press</a>, Copyright 2023</p>
</div>
  </div></div></div></div>            <p class="attribute-creator"></p>
          </footer>
        </div>
      </div>
    
  </div>
</div>
  </body>
</html>
